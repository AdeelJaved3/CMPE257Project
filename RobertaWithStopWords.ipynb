{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base package for the tasks from pytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/anusha/Code/Python/Project_Local/FinalWIthStopWords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "3    7998\n",
       "1    7992\n",
       "4    7973\n",
       "2    7961\n",
       "0    7945\n",
       "5    7823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <td>47692.0</td>\n",
       "      <td>2.493395</td>\n",
       "      <td>1.703893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count      mean       std  min  25%  50%  75%  max\n",
       "cyberbullying_type  47692.0  2.493395  1.703893  0.0  1.0  2.0  4.0  5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <td>47473.0</td>\n",
       "      <td>2.490321</td>\n",
       "      <td>1.69975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count      mean      std  min  25%  50%  75%  max\n",
       "cyberbullying_type  47473.0  2.490321  1.69975  0.0  1.0  2.0  4.0  5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=0)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet_text']\n",
    "y = df['cyberbullying_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X_train.values\n",
    "labels = y_train.values\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "3    7198\n",
       "1    7193\n",
       "2    7164\n",
       "4    7162\n",
       "0    7081\n",
       "5    6927\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  misss r came out as a lee's bIAN because the boys at nasa were picking on me after that girl gave me that valentines note she was telling me of her time being bullied at high sxhool\n",
      "Tokenized:  ['miss', 's', 'Ġr', 'Ġcame', 'Ġout', 'Ġas', 'Ġa', 'Ġle', 'e', \"'s\", 'Ġb', 'IAN', 'Ġbecause', 'Ġthe', 'Ġboys', 'Ġat', 'Ġn', 'asa', 'Ġwere', 'Ġpicking', 'Ġon', 'Ġme', 'Ġafter', 'Ġthat', 'Ġgirl', 'Ġgave', 'Ġme', 'Ġthat', 'Ġval', 'ent', 'ines', 'Ġnote', 'Ġshe', 'Ġwas', 'Ġtelling', 'Ġme', 'Ġof', 'Ġher', 'Ġtime', 'Ġbeing', 'Ġbullied', 'Ġat', 'Ġhigh', 'Ġs', 'x', 'h', 'ool']\n",
      "Token IDs:  [17745, 29, 910, 376, 66, 25, 10, 2084, 242, 18, 741, 10296, 142, 5, 2786, 23, 295, 8810, 58, 6201, 15, 162, 71, 14, 1816, 851, 162, 14, 7398, 1342, 3141, 1591, 79, 21, 2758, 162, 9, 69, 86, 145, 21745, 23, 239, 579, 1178, 298, 8110]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  919\n"
     ]
    }
   ],
   "source": [
    "# Find maximum length based on our dataset\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store outputs\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #    (1) Tokenize the sentence\n",
    "    #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "    #    (3) Truncate/Pad sentence to max length\n",
    "    #    (4) Map tokens to their IDs\n",
    "    #    (5) Create attention mask\n",
    "    #    (6) Return a dictionary of outputs\n",
    "    encoded_sent = tokenizer.encode_plus(\n",
    "        text=sent,  # Preprocess sentence\n",
    "        add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "        max_length=150,             # Max length to truncate/pad\n",
    "        pad_to_max_length=True,         # Pad sentence to max length\n",
    "        return_attention_mask=True,      # Return attention mask\n",
    "        truncation = True\n",
    "        )\n",
    "    # Add the outputs to the lists\n",
    "    input_ids.append(encoded_sent.get('input_ids'))\n",
    "    attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  misss r came out as a lee's bIAN because the boys at nasa were picking on me after that girl gave me that valentines note she was telling me of her time being bullied at high sxhool\n",
      "Token IDs: tensor([    0, 17745,    29,   910,   376,    66,    25,    10,  2084,   242,\n",
      "           18,   741, 10296,   142,     5,  2786,    23,   295,  8810,    58,\n",
      "         6201,    15,   162,    71,    14,  1816,   851,   162,    14,  7398,\n",
      "         1342,  3141,  1591,    79,    21,  2758,   162,     9,    69,    86,\n",
      "          145, 21745,    23,   239,   579,  1178,   298,  8110,     2,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use 90% for training and 10% for validation\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks, random_state=2018, test_size=0.1, stratify=labels)\n",
    "\n",
    "train_inputs = input_ids\n",
    "train_labels = labels\n",
    "train_masks = attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "#validation_dataset = TensorDataset(validation_inputs, validation_masks, validation_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7081, 7193, 7164, 7198, 7162, 6927])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class_sample_count = np.unique(train_labels, return_counts=True)[1]\n",
    "\n",
    "class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = weight[train_labels]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = sampler, # Select batches based on weights\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "# validation_dataloader = DataLoader(\n",
    "#             validation_dataset, # The validation samples.\n",
    "#             sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", \n",
    "    # Specify number of classes\n",
    "    num_labels = 6, \n",
    "    # Whether the model returns attentions weights\n",
    "    output_attentions = False,\n",
    "    # Whether the model returns all hidden-states \n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "type(device)\n",
    "\n",
    "device\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,336.    Elapsed: 0:05:54.\n",
      "  Batch    80  of  1,336.    Elapsed: 0:12:06.\n",
      "  Batch   120  of  1,336.    Elapsed: 0:17:53.\n",
      "  Batch   160  of  1,336.    Elapsed: 0:23:28.\n",
      "  Batch   200  of  1,336.    Elapsed: 0:29:17.\n",
      "  Batch   240  of  1,336.    Elapsed: 0:35:09.\n",
      "  Batch   280  of  1,336.    Elapsed: 0:42:41.\n",
      "  Batch   320  of  1,336.    Elapsed: 0:48:24.\n",
      "  Batch   360  of  1,336.    Elapsed: 0:54:15.\n",
      "  Batch   400  of  1,336.    Elapsed: 1:00:08.\n",
      "  Batch   440  of  1,336.    Elapsed: 1:06:00.\n",
      "  Batch   480  of  1,336.    Elapsed: 1:11:49.\n",
      "  Batch   520  of  1,336.    Elapsed: 1:17:50.\n",
      "  Batch   560  of  1,336.    Elapsed: 1:24:19.\n",
      "  Batch   600  of  1,336.    Elapsed: 1:31:08.\n",
      "  Batch   640  of  1,336.    Elapsed: 1:37:14.\n",
      "  Batch   680  of  1,336.    Elapsed: 1:43:13.\n",
      "  Batch   720  of  1,336.    Elapsed: 1:49:09.\n",
      "  Batch   760  of  1,336.    Elapsed: 1:55:00.\n",
      "  Batch   800  of  1,336.    Elapsed: 2:01:39.\n",
      "  Batch   840  of  1,336.    Elapsed: 2:11:49.\n",
      "  Batch   880  of  1,336.    Elapsed: 2:22:11.\n",
      "  Batch   920  of  1,336.    Elapsed: 2:33:46.\n",
      "  Batch   960  of  1,336.    Elapsed: 2:40:32.\n",
      "  Batch 1,000  of  1,336.    Elapsed: 2:46:33.\n",
      "  Batch 1,040  of  1,336.    Elapsed: 2:52:29.\n",
      "  Batch 1,080  of  1,336.    Elapsed: 2:57:56.\n",
      "  Batch 1,120  of  1,336.    Elapsed: 3:02:53.\n",
      "  Batch 1,160  of  1,336.    Elapsed: 3:08:24.\n",
      "  Batch 1,200  of  1,336.    Elapsed: 3:13:41.\n",
      "  Batch 1,240  of  1,336.    Elapsed: 3:19:30.\n",
      "  Batch 1,280  of  1,336.    Elapsed: 3:25:44.\n",
      "  Batch 1,320  of  1,336.    Elapsed: 3:30:04.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 3:31:48\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,336.    Elapsed: 0:04:28.\n",
      "  Batch    80  of  1,336.    Elapsed: 0:08:45.\n",
      "  Batch   120  of  1,336.    Elapsed: 0:13:04.\n",
      "  Batch   160  of  1,336.    Elapsed: 0:17:42.\n",
      "  Batch   200  of  1,336.    Elapsed: 0:22:02.\n",
      "  Batch   240  of  1,336.    Elapsed: 0:26:59.\n",
      "  Batch   280  of  1,336.    Elapsed: 0:31:48.\n",
      "  Batch   320  of  1,336.    Elapsed: 0:36:07.\n",
      "  Batch   360  of  1,336.    Elapsed: 0:40:30.\n",
      "  Batch   400  of  1,336.    Elapsed: 0:44:41.\n",
      "  Batch   440  of  1,336.    Elapsed: 0:48:54.\n",
      "  Batch   480  of  1,336.    Elapsed: 0:53:07.\n",
      "  Batch   520  of  1,336.    Elapsed: 0:57:19.\n",
      "  Batch   560  of  1,336.    Elapsed: 1:01:29.\n",
      "  Batch   600  of  1,336.    Elapsed: 1:05:38.\n",
      "  Batch   640  of  1,336.    Elapsed: 1:09:51.\n",
      "  Batch   680  of  1,336.    Elapsed: 1:14:01.\n",
      "  Batch   720  of  1,336.    Elapsed: 1:18:10.\n",
      "  Batch   760  of  1,336.    Elapsed: 1:22:19.\n",
      "  Batch   800  of  1,336.    Elapsed: 1:26:32.\n",
      "  Batch   840  of  1,336.    Elapsed: 1:30:41.\n",
      "  Batch   880  of  1,336.    Elapsed: 1:34:51.\n",
      "  Batch   920  of  1,336.    Elapsed: 1:39:04.\n",
      "  Batch   960  of  1,336.    Elapsed: 1:43:12.\n",
      "  Batch 1,000  of  1,336.    Elapsed: 1:47:28.\n",
      "  Batch 1,040  of  1,336.    Elapsed: 1:51:40.\n",
      "  Batch 1,080  of  1,336.    Elapsed: 1:55:51.\n",
      "  Batch 1,120  of  1,336.    Elapsed: 2:00:02.\n",
      "  Batch 1,160  of  1,336.    Elapsed: 2:04:12.\n",
      "  Batch 1,200  of  1,336.    Elapsed: 2:08:27.\n",
      "  Batch 1,240  of  1,336.    Elapsed: 2:12:42.\n",
      "  Batch 1,280  of  1,336.    Elapsed: 2:16:55.\n",
      "  Batch 1,320  of  1,336.    Elapsed: 2:21:06.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epcoh took: 2:22:41\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,336.    Elapsed: 0:04:13.\n",
      "  Batch    80  of  1,336.    Elapsed: 0:08:23.\n",
      "  Batch   120  of  1,336.    Elapsed: 0:12:30.\n",
      "  Batch   160  of  1,336.    Elapsed: 0:16:40.\n",
      "  Batch   200  of  1,336.    Elapsed: 0:20:47.\n",
      "  Batch   240  of  1,336.    Elapsed: 0:24:59.\n",
      "  Batch   280  of  1,336.    Elapsed: 0:29:08.\n",
      "  Batch   320  of  1,336.    Elapsed: 0:33:21.\n",
      "  Batch   360  of  1,336.    Elapsed: 0:37:30.\n",
      "  Batch   400  of  1,336.    Elapsed: 0:41:38.\n",
      "  Batch   440  of  1,336.    Elapsed: 0:45:51.\n",
      "  Batch   480  of  1,336.    Elapsed: 0:50:00.\n",
      "  Batch   520  of  1,336.    Elapsed: 0:54:08.\n",
      "  Batch   560  of  1,336.    Elapsed: 0:58:16.\n",
      "  Batch   600  of  1,336.    Elapsed: 1:02:25.\n",
      "  Batch   640  of  1,336.    Elapsed: 1:06:39.\n",
      "  Batch   680  of  1,336.    Elapsed: 1:10:47.\n",
      "  Batch   720  of  1,336.    Elapsed: 1:15:10.\n",
      "  Batch   760  of  1,336.    Elapsed: 1:19:22.\n",
      "  Batch   800  of  1,336.    Elapsed: 1:23:32.\n",
      "  Batch   840  of  1,336.    Elapsed: 1:27:42.\n",
      "  Batch   880  of  1,336.    Elapsed: 1:31:53.\n",
      "  Batch   920  of  1,336.    Elapsed: 1:36:04.\n",
      "  Batch   960  of  1,336.    Elapsed: 1:40:15.\n",
      "  Batch 1,000  of  1,336.    Elapsed: 1:44:23.\n",
      "  Batch 1,040  of  1,336.    Elapsed: 1:48:35.\n",
      "  Batch 1,080  of  1,336.    Elapsed: 1:52:43.\n",
      "  Batch 1,120  of  1,336.    Elapsed: 1:56:55.\n",
      "  Batch 1,160  of  1,336.    Elapsed: 2:01:05.\n",
      "  Batch 1,200  of  1,336.    Elapsed: 2:05:22.\n",
      "  Batch 1,240  of  1,336.    Elapsed: 2:09:33.\n",
      "  Batch 1,280  of  1,336.    Elapsed: 2:13:44.\n",
      "  Batch 1,320  of  1,336.    Elapsed: 2:17:55.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 2:19:33\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,336.    Elapsed: 0:04:05.\n",
      "  Batch    80  of  1,336.    Elapsed: 0:08:08.\n",
      "  Batch   120  of  1,336.    Elapsed: 0:12:19.\n",
      "  Batch   160  of  1,336.    Elapsed: 0:16:20.\n",
      "  Batch   200  of  1,336.    Elapsed: 0:21:53.\n",
      "  Batch   240  of  1,336.    Elapsed: 0:27:15.\n",
      "  Batch   280  of  1,336.    Elapsed: 0:31:30.\n",
      "  Batch   320  of  1,336.    Elapsed: 0:36:16.\n",
      "  Batch   360  of  1,336.    Elapsed: 0:40:54.\n",
      "  Batch   400  of  1,336.    Elapsed: 0:45:49.\n",
      "  Batch   440  of  1,336.    Elapsed: 0:50:24.\n",
      "  Batch   480  of  1,336.    Elapsed: 0:55:16.\n",
      "  Batch   520  of  1,336.    Elapsed: 1:00:03.\n",
      "  Batch   560  of  1,336.    Elapsed: 1:04:49.\n",
      "  Batch   600  of  1,336.    Elapsed: 1:09:33.\n",
      "  Batch   640  of  1,336.    Elapsed: 1:13:53.\n",
      "  Batch   680  of  1,336.    Elapsed: 1:18:12.\n",
      "  Batch   720  of  1,336.    Elapsed: 1:23:02.\n",
      "  Batch   760  of  1,336.    Elapsed: 1:27:21.\n",
      "  Batch   800  of  1,336.    Elapsed: 1:32:08.\n",
      "  Batch   840  of  1,336.    Elapsed: 1:37:39.\n",
      "  Batch   880  of  1,336.    Elapsed: 1:43:27.\n",
      "  Batch   920  of  1,336.    Elapsed: 1:48:05.\n",
      "  Batch   960  of  1,336.    Elapsed: 1:52:39.\n",
      "  Batch 1,000  of  1,336.    Elapsed: 1:57:04.\n",
      "  Batch 1,040  of  1,336.    Elapsed: 2:01:33.\n",
      "  Batch 1,080  of  1,336.    Elapsed: 2:05:53.\n",
      "  Batch 1,120  of  1,336.    Elapsed: 2:10:08.\n",
      "  Batch 1,160  of  1,336.    Elapsed: 2:14:20.\n",
      "  Batch 1,200  of  1,336.    Elapsed: 2:18:31.\n",
      "  Batch 1,240  of  1,336.    Elapsed: 2:22:44.\n",
      "  Batch 1,280  of  1,336.    Elapsed: 2:26:59.\n",
      "  Batch 1,320  of  1,336.    Elapsed: 2:31:12.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 2:32:51\n",
      "\n",
      "Training complete!\n",
      "Total training took 10:46:53 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        tmp = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss, logits = tmp[0], tmp[1]\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # # ========================================\n",
    "    # #               Validation\n",
    "    # # ========================================\n",
    "    # # After the completion of each training epoch, measure our performance on\n",
    "    # # our validation set.\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"Running Validation...\")\n",
    "\n",
    "    # t0 = time.time()\n",
    "\n",
    "    # # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # # during evaluation.\n",
    "    # model.eval()\n",
    "\n",
    "    # # Tracking variables \n",
    "    # total_eval_accuracy = 0\n",
    "    # total_eval_loss = 0\n",
    "    # nb_eval_steps = 0\n",
    "\n",
    "    # # Evaluate data for one epoch\n",
    "    # for batch in validation_dataloader:\n",
    "        \n",
    "    #     # Unpack this training batch from our dataloader. \n",
    "    #     #\n",
    "    #     # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "    #     # the `to` method.\n",
    "    #     #\n",
    "    #     # `batch` contains three pytorch tensors:\n",
    "    #     #   [0]: input ids \n",
    "    #     #   [1]: attention masks\n",
    "    #     #   [2]: labels \n",
    "    #     b_input_ids = batch[0].to(device)\n",
    "    #     b_input_mask = batch[1].to(device)\n",
    "    #     b_labels = batch[2].to(device)\n",
    "        \n",
    "    #     # Tell pytorch not to bother with constructing the compute graph during\n",
    "    #     # the forward pass, since this is only needed for backprop (training).\n",
    "    #     with torch.no_grad():        \n",
    "\n",
    "    #         # Forward pass, calculate logit predictions.\n",
    "    #         # token_type_ids is the same as the \"segment ids\", which \n",
    "    #         # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "    #         # The documentation for this `model` function is here: \n",
    "    #         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "    #         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "    #         # values prior to applying an activation function like the softmax.\n",
    "    #         temp = model(b_input_ids, \n",
    "    #                                token_type_ids=None, \n",
    "    #                                attention_mask=b_input_mask,\n",
    "    #                                labels=b_labels)\n",
    "    #         (loss, logits) = temp[0], temp[1]\n",
    "            \n",
    "    #     # Accumulate the validation loss.\n",
    "    #     total_eval_loss += loss.item()\n",
    "\n",
    "    #     # Move logits and labels to CPU\n",
    "    #     logits = logits.detach().cpu().numpy()\n",
    "    #     label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    #     # Calculate the accuracy for this batch of test sentences, and\n",
    "    #     # accumulate it over all batches.\n",
    "    #     total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # # Report the final accuracy for this validation run.\n",
    "    # avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # # Calculate the average loss over all of the batches.\n",
    "    # avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # # Measure how long the validation run took.\n",
    "    # validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    # print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/Users/anusha/Code/Python/Project_Local/robertamodelWithouStopWords')\n",
    "saved_model = torch.load('/Users/anusha/Code/Python/Project_Local/robertamodelWithouStopWords')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.43019997</td>\n",
       "      <td>3:31:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.29769193</td>\n",
       "      <td>2:22:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24410541</td>\n",
       "      <td>2:19:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.21561661</td>\n",
       "      <td>2:32:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss Training Time\n",
       "epoch                             \n",
       "1         0.43019997       3:31:48\n",
       "2         0.29769193       2:22:41\n",
       "3         0.24410541       2:19:33\n",
       "4         0.21561661       2:32:51"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('display.precision', 8)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "# plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = X_test.values\n",
    "y_true = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "@param    data (np.array): Array of texts to be processed.\n",
    "@return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "@return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "              tokens should be attended to by the model.\n",
    "\"\"\"\n",
    "# Create empty lists to store outputs\n",
    "t_input_ids = []\n",
    "t_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for s in pred_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #    (1) Tokenize the sentence\n",
    "    #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "    #    (3) Truncate/Pad sentence to max length\n",
    "    #    (4) Map tokens to their IDs\n",
    "    #    (5) Create attention mask\n",
    "    #    (6) Return a dictionary of outputs\n",
    "    p_encoded_sent = tokenizer.encode_plus(\n",
    "        text=s,  # Preprocess sentence\n",
    "        add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "        max_length=150,             # Max length to truncate/pad\n",
    "        pad_to_max_length=True,         # Pad sentence to max length\n",
    "        return_attention_mask=True,      # Return attention mask\n",
    "        truncation = True\n",
    "        )\n",
    "    # Add the outputs to the lists\n",
    "    t_input_ids.append(p_encoded_sent.get('input_ids'))\n",
    "    t_attention_masks.append(p_encoded_sent.get('attention_mask'))\n",
    "# Convert lists to tensors\n",
    "t_input_ids = torch.tensor(t_input_ids)\n",
    "t_attention_masks = torch.tensor(t_attention_masks)\n",
    "t_labels = torch.tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4748, 150])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "prediction_data = TensorDataset(t_input_ids, t_attention_masks, t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "\n",
    "# For Prediction the order doesn't matter, so we'll just read them sequentially.\n",
    "prediction_dataloader = DataLoader(\n",
    "            prediction_data, # The test samples.\n",
    "            sampler = SequentialSampler(prediction_data), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('/Users/anusha/Code/Python/Project_Local/robertamodelWithouStopWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 4,748 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.36%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/anusha/Code/Python/Project_Local/RobertaWithStopWords.ipynb Cell 40\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anusha/Code/Python/Project_Local/RobertaWithStopWords.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(flat_true_labels, flat_predictions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anusha/Code/Python/Project_Local/RobertaWithStopWords.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anusha/Code/Python/Project_Local/RobertaWithStopWords.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m f1 \u001b[39m=\u001b[39m f1_score(flat_true_labels, flat_predictions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anusha/Code/Python/Project_Local/RobertaWithStopWords.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mF1 score: \u001b[39m\u001b[39m{\u001b[39;00mf1\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anusha/Code/Python/Project_Local/RobertaWithStopWords.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m precision \u001b[39m=\u001b[39m precision_score(flat_true_labels, flat_predictions)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m   1071\u001b[0m     {\n\u001b[1;32m   1072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1097\u001b[0m ):\n\u001b[1;32m   1098\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1239\u001b[0m         y_true,\n\u001b[1;32m   1240\u001b[0m         y_pred,\n\u001b[1;32m   1241\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1242\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1243\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1244\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1245\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1246\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1247\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m   1251\u001b[0m     {\n\u001b[1;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1279\u001b[0m ):\n\u001b[1;32m   1280\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \n\u001b[1;32m   1282\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[39m    0.38...\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1411\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1412\u001b[0m         y_true,\n\u001b[1;32m   1413\u001b[0m         y_pred,\n\u001b[1;32m   1414\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   1415\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1416\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1417\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1418\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1419\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1420\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1421\u001b[0m     )\n\u001b[1;32m   1422\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \n\u001b[1;32m   1565\u001b[0m \u001b[39mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[39m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m zero_division_value \u001b[39m=\u001b[39m _check_zero_division(zero_division)\n\u001b[0;32m-> 1721\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1723\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1516\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1515\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1520\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[1;32m   1521\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1522\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1523\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "f1 = f1_score(flat_true_labels, flat_predictions)\n",
    "print(f'F1 score: {f1*100:.2f}%')\n",
    "precision = precision_score(flat_true_labels, flat_predictions)\n",
    "print(f'Precision: {precision*100:.2f}%')\n",
    "recall = recall_score(flat_true_labels, flat_predictions)\n",
    "print(f'Recall: {recall*100:.2f}%')\n",
    "\n",
    "c_matrix =confusion_matrix(flat_true_labels,flat_predictions)\n",
    "clr = classification_report(flat_true_labels,flat_predictions)\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print(\"Accuracy\", round(accuracy,2) )\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(c_matrix,annot=True, fmt='g', vmin=0, cbar=False, cmap='Blues')\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Actual Value\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()\n",
    "print(clr)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
