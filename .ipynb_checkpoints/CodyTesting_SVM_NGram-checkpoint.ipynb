{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocessor as p\n",
    "import pycld2\n",
    "import pickle as pkl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop word set that does not include any pronouns and any negating words (e.g. don't, wouldn't)\n",
    "custom_stopwords = {\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"have\", \"has\", \"had\", \"having\",\n",
    "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
    "    \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\",\n",
    "    \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "    \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"in\", \"out\",\n",
    "    \"on\", \"over\", \"under\", \"again\", \"then\", \"once\", \"when\", \"where\",\n",
    "    \"why\", \"how\", \"all\", \"any\", \"both\", \"more\", \"most\", \"other\",\n",
    "    \"some\", \"such\", \"no\", \"not\", \"only\", \"own\", \"so\", \"than\", \"too\", \"very\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_test, y_pred):\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [\"non-cyberbullying\", \"cyberbullying\"])\n",
    "    plt.yticks(tick_marks, [\"non-cyberbullying\", \"cyberbullying\"])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i, j]), horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # This effectively removes all special characters\n",
    "    text = p.clean(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "#     words = text.split()\n",
    "\n",
    "#     # remove leading and trailing white spaces with strip()\n",
    "#     # remove custom stopwords from text\n",
    "#     words = [word.strip() for word in words if word.strip() not in custom_stopwords]\n",
    "#     text = ' '.join(words)\n",
    "\n",
    "    # Replace multiple consecutive white spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.isna().sum()\n",
    "    print(f'Missing Values:\\n{missing_values}\\n')\n",
    "\n",
    "    # Find all tweets that are less than 3 words and remove them\n",
    "    df['tweet_length'] = [len(text.split()) for text in df.tweet_text]\n",
    "    df = df.loc[df['tweet_length'] >= 3]\n",
    "    df.drop('tweet_length', axis=1, inplace=True)\n",
    "\n",
    "    # Clean the text by removing special characters and converting all text to lower case\n",
    "    df['tweet_text'] = df['tweet_text'].map(clean_text)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_encode(df):\n",
    "\n",
    "    df = df.loc[df['tweet_text'].apply(lambda x: safe_detect(x) == 'en')]\n",
    "    \n",
    "    # Data balancing \n",
    "    # Sample 3,325 observation for each category if stop words are removed\n",
    "    # Sample 3,450 observation for each category if stop words are not removed\n",
    "    sample_religion = df.loc[df['cyberbullying_type']=='religion'].sample\\\n",
    "        (n=3450, random_state=42)\n",
    "\n",
    "    sample_age = df.loc[df['cyberbullying_type']=='age'].sample\\\n",
    "        (n=3450, random_state=42)\n",
    "\n",
    "    sample_other_cyberbullying = df.loc[df['cyberbullying_type']=='other_cyberbullying'].sample\\\n",
    "        (n=3450, random_state=42)\n",
    "\n",
    "    sample_ethnicity = df.loc[df['cyberbullying_type']=='ethnicity'].sample\\\n",
    "        (n=3450, random_state=42)\n",
    "\n",
    "    sample_gender = df.loc[df['cyberbullying_type']=='gender'].sample\\\n",
    "        (n=3450, random_state=42)\n",
    "\n",
    "    samples_df = pd.concat([sample_religion,sample_age,\n",
    "                            sample_other_cyberbullying,sample_ethnicity,sample_gender])\n",
    "\n",
    "    # Manually encode the different types of cyberbullying/non-cyberbullying\n",
    "    samples_df.loc[:, 'cyberbullying_type'] = 1\n",
    "\n",
    "    df.loc[df['cyberbullying_type'] == 'not_cyberbullying', 'cyberbullying_type'] = 0\n",
    "    non_cyberbullying_df = df.loc[df['cyberbullying_type']==0].copy()\n",
    "\n",
    "    # Ensure target column is of type int\n",
    "    non_cyberbullying_df['cyberbullying_type'] = non_cyberbullying_df['cyberbullying_type']\\\n",
    "        .astype(int)\n",
    "    samples_df['cyberbullying_type'] = samples_df['cyberbullying_type'].astype(int)\n",
    "\n",
    "    df = pd.concat([non_cyberbullying_df,samples_df])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_detect(text):\n",
    "    try:\n",
    "        return pycld2.detect(text)[2][0][1]\n",
    "    except Exception:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    data = load_pkl_data('./data 2/formspring_data.pkl')\n",
    "    formspring_df = pd.DataFrame(data)\n",
    "\n",
    "    # Check the raw formspring data\n",
    "    print(f'Check raw formspring_df: {formspring_df.head(5)}\\n')\n",
    "    formspring_df.rename(columns={'text':'tweet_text', 'label':\n",
    "        'cyberbullying_type'}, inplace=True)\n",
    "    formspring_df = preprocess_data(formspring_df)\n",
    "\n",
    "    # Remove any non english entries\n",
    "    formspring_df = formspring_df.loc[formspring_df['tweet_text'].apply(lambda x: safe_detect(x) == 'en')]\n",
    "\n",
    "    # Check cleaned formspring data\n",
    "    print(f'Check cleaned formspring_df: {formspring_df.head(5)}\\n')\n",
    "\n",
    "    df = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "\n",
    "    # Check raw cyberbullying tweet data\n",
    "    print(f'Check raw cyberbullying tweet data: {df.head(5)}\\n')\n",
    "\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Check cleaned cyberbullying tweet data\n",
    "    print(f'Check cleaned cyberbullying tweet data: {df.head(5)}\\n')\n",
    "    df = sample_encode(df)\n",
    "\n",
    "    # Merge both dataframes for data balancing\n",
    "    merged_df = pd.concat([df, formspring_df])\n",
    "\n",
    "    # Check merged dataframe data\n",
    "    # print(f'Check merged dataframe {merged_df.head(10)}')\n",
    "    # Check Unique y values\n",
    "    y_values = merged_df['cyberbullying_type'].value_counts()\n",
    "    \n",
    "    for category, count in y_values.items():\n",
    "        print(f'Y value: {category}, Count: {count}')\n",
    "    \n",
    "    print('\\n')\n",
    "    # X is the features (the input data)\n",
    "    X = merged_df['tweet_text']\n",
    "\n",
    "    # y is the targets (output or label)\n",
    "    y = merged_df['cyberbullying_type']\n",
    "\n",
    "    # Create features based frequency of individual words in a given\n",
    "    # observation\n",
    "    \n",
    "    # recent 84.7 with 2 ngams and no stop words removed \n",
    "    # Additional ngrams reduced accuracy \n",
    "    # Removing stop words reduced accuracy \n",
    "    ngram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "    X = ngram_vectorizer.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y, test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    \n",
    "    SVM = svm.SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "#     SVM = svm.SVC(kernel='linear',decision_function_shape='ovr')\n",
    "\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = SVM.predict(X_test)\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "    # Calculate and print individual metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
